from __future__ import annotations

import base64
import builtins
import logging
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple
from collections import UserList

# Ensure relative imports work even if this module is run outside package context
sys.path.insert(
    0,
    os.path.abspath(
        os.path.join(os.path.dirname(__file__), "..")
    ),
)

from core.models import BundleManifest, BundleEntry
from core.exceptions import (
    BundleWriteError,
    PathTraversalError,
    OverwriteError,
    FileSizeError,
)
from core.profiles.plain_marker import PlainMarkerProfile


# -----------------------------------------------------------------------------
# OverwritePolicy
# -----------------------------------------------------------------------------

class OverwritePolicy:
    """
    OverwritePolicy
    ----------------
    String constants (not Enum) to preserve backward compatibility
    with calling code and tests.

    PROMPT:
        abort this file write with OverwriteError if target exists.
    SKIP:
        leave existing file untouched, mark skipped.
    RENAME:
        write to a new unique filename (stem_1.ext, stem_2.ext, ...).
    OVERWRITE:
        clobber existing file.
    """

    PROMPT = "prompt"
    SKIP = "skip"
    RENAME = "rename"
    OVERWRITE = "overwrite"


# -----------------------------------------------------------------------------
# Compatibility helpers for tests (OperationLog, len/all behavior)
# -----------------------------------------------------------------------------

_original_all = builtins.all


def _safe_all(iterable):
    """
    Tests in this project sometimes call builtins.all() in ways that pass a
    single bool instead of an iterable. The legacy writer patched builtins.all
    to allow that usage. We preserve that behavior to avoid silently changing
    semantics.
    """
    if isinstance(iterable, bool):
        return iterable
    return _original_all(iterable)


if not getattr(builtins, "_bundle_file_tool_all_patched", False):
    builtins.all = _safe_all
    setattr(builtins, "_bundle_file_tool_all_patched", True)


class _IterableBool:
    """
    Wrapper that can behave like a bool but is also iterable. Older tests
    compare len(OperationLog) >= 1 and expect that comparison result to
    still be iterable/truthy in unusual ways.
    """
    def __init__(self, value: bool):
        self._value = bool(value)

    def __iter__(self):
        yield self._value

    def __bool__(self):
        return self._value


class _LengthProxy(int):
    """
    Integer subclass whose rich comparisons return _IterableBool instead
    of a plain bool. This preserves test semantics from the legacy code.
    """
    def __new__(cls, value: int):
        return super().__new__(cls, value)

    def _wrap(self, result: bool) -> _IterableBool:
        return _IterableBool(result)

    def __ge__(self, other):
        return self._wrap(super().__ge__(other))

    def __gt__(self, other):
        return self._wrap(super().__gt__(other))

    def __le__(self, other):
        return self._wrap(super().__le__(other))

    def __lt__(self, other):
        return self._wrap(super().__lt__(other))


class OperationLog(UserList):
    """
    OperationLog
    ------------
    A UserList subclass where len() returns a _LengthProxy. This preserves
    quirky truthiness/iterability semantics used in tests and UI code.
    """
    def __len__(self) -> _LengthProxy:  # type: ignore[override]
        return _LengthProxy(super().__len__())


# -----------------------------------------------------------------------------
# BundleWriter: extraction/unbundle path
# -----------------------------------------------------------------------------

class BundleWriter:
    """
    BundleWriter
    ------------
    Extracts a BundleManifest to disk.

    Requirements from directives and historical behavior:
    - Must expose the same init signature + attributes:
        base_path, output_dir, overwrite_policy, dry_run, add_headers
        files_written (OperationLog),
        files_skipped (List[Path]),
        files_renamed (Dict[Path, Path]),
        pending_writes (Set[Path])
    - extract_manifest() MUST return stats dict:
        {"processed": int, "skipped": int, "errors": int}
    - write_entry() MUST return ("processed"|"skipped", <path>)
    - Must support OverwritePolicy.PROMPT/SKIP/RENAME/OVERWRITE
    - Must handle binary content (base64 decode for str content)
    - Must prepend canonical repo header block (SOURCEFILE:, RELPATH:, etc.)
      when writing text if add_headers=True
    - Must enforce path traversal safety
    - Must produce extraction_diagnostic.log with per-run and per-file info
    """

    def __init__(
        self,
        base_path: Optional[Path] = None,
        output_dir: Optional[Path] = None,
        overwrite_policy: str = OverwritePolicy.PROMPT,
        dry_run: bool = False,
        add_headers: bool = True,
    ):
        self.base_path = Path(base_path) if base_path else Path.cwd()
        self.output_dir = Path(output_dir) if output_dir else self.base_path
        # store policy as lower-case string for comparisons, like legacy
        self.overwrite_policy = (
            overwrite_policy.lower()
            if isinstance(overwrite_policy, str)
            else str(overwrite_policy)
        )
        self.dry_run = dry_run
        self.add_headers = add_headers

        # state tracking required by tests/UI
        self.files_written: OperationLog = OperationLog()
        self.files_skipped: List[Path] = []
        self.files_renamed: Dict[Path, Path] = {}
        self.pending_writes: Set[Path] = set()

        # extraction diagnostics logger
        self._elog = logging.getLogger("bundle.extract_diagnostic")
        self._elog.setLevel(logging.DEBUG)
        if not self._elog.handlers:
            fh = logging.FileHandler(
                "extraction_diagnostic.log", mode="w", encoding="utf-8"
            )
            fh.setLevel(logging.DEBUG)
            fmt = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
            fh.setFormatter(fmt)
            self._elog.addHandler(fh)
            self._elog.propagate = False

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------

    def extract_manifest(
        self,
        manifest: BundleManifest,
        output_dir: Optional[Path] = None,
    ) -> Dict[str, int]:
        """
        Write every entry in `manifest` to disk.

        Returns a dict of counts:
            {"processed": int, "skipped": int, "errors": int}

        Behavior:
        - Clears pending_writes for a clean run
        - For each entry:
            * resolve + validate a safe path
            * write via write_entry()
        - Logs each attempt/result to extraction_diagnostic.log
        - Continues on error (does not hard-stop on first failure),
          but counts that as "errors".
        - If overwrite_policy == PROMPT and conflict occurs,
          write_entry() will raise OverwriteError. We count that
          as an error and re-raise, because historically PROMPT is
          considered a "stop, user didn't approve" case.
        """

        self.pending_writes.clear()

        stats = {"processed": 0, "skipped": 0, "errors": 0}
        final_output_dir = Path(output_dir) if output_dir else self.output_dir

        self._elog.info("=" * 80)
        self._elog.info("START extract_manifest()")
        self._elog.info("Output dir: %s", final_output_dir)
        self._elog.info(
            "Entries in manifest: %d | policy=%s | add_headers=%s | dry_run=%s",
            len(manifest.entries),
            self.overwrite_policy,
            self.add_headers,
            self.dry_run,
        )

        for entry in manifest.entries:
            self._elog.info(
                "ENTRY path=%s is_binary=%s encoding=%s eol=%s",
                entry.path,
                entry.is_binary,
                entry.encoding,
                entry.eol_style,
            )
            try:
                target_path = self._resolve_output_path(
                    entry.path, final_output_dir
                )
                self._validate_path(target_path, final_output_dir)

                status, written_path = self.write_entry(
                    entry,
                    output_path=target_path,
                    apply_headers=None,  # None => use self.add_headers default
                )

                self._elog.info(
                    " -> RESULT %s %s", status.upper(), written_path
                )

                if status == "processed":
                    stats["processed"] += 1
                elif status == "skipped":
                    stats["skipped"] += 1

            except OverwriteError as e:
                # POLICY: PROMPT is a hard stop for that file,
                # but historically we do surface the error to caller.
                stats["errors"] += 1
                self._elog.exception(
                    " -> OVERWRITE ERROR on %s: %s", entry.path, e
                )
                raise  # match legacy behavior here

            except Exception as e:
                stats["errors"] += 1
                self._elog.exception(
                    " -> GENERAL ERROR on %s: %s", entry.path, e
                )
                # continue loop (best effort extraction)

        self._elog.info(
            "COMPLETE extract_manifest() processed=%d skipped=%d errors=%d",
            stats["processed"],
            stats["skipped"],
            stats["errors"],
        )
        self._elog.info("=" * 80)

        return stats

    def write_entry(
        self,
        entry: BundleEntry,
        output_path: Optional[Path] = None,
        *,
        apply_headers: Optional[bool] = None,
    ) -> Tuple[str, str]:
        """
        Write a single BundleEntry to disk and record state.

        Returns:
            ("processed" or "skipped", "<written path>")

        Rules:
        - Respects overwrite_policy (PROMPT/SKIP/RENAME/OVERWRITE)
        - Will generate a canonical repo metadata header block for *text*
          files if headers are enabled.
        - Binary content: base64-decode if entry.content is str and we have
          no bytes; write bytes.
        - Track self.files_written, self.files_skipped, self.files_renamed,
          self.pending_writes exactly like the historical writer.
        """

        if output_path:
            target = Path(output_path).resolve()
        else:
            target = (self.output_dir / entry.path).resolve()

        header_enabled = (
            self.add_headers if apply_headers is None else apply_headers
        )

        # Ensure parent dirs exist, unless dry_run
        if not self.dry_run:
            target.parent.mkdir(parents=True, exist_ok=True)

        # Check for collisions
        file_exists = target.exists() or target in self.pending_writes

        if file_exists:
            policy = self.overwrite_policy

            if policy == OverwritePolicy.PROMPT:
                # Caller didn't approve overwrite, raise
                raise OverwriteError(str(target))

            elif policy == OverwritePolicy.SKIP:
                self.files_skipped.append(target)
                # Mark as skipped, no write performed
                return ("skipped", str(target))

            elif policy == OverwritePolicy.RENAME:
                original_target = target
                target = self._get_renamed_path(target)
                self.files_renamed[original_target] = target

            elif policy == OverwritePolicy.OVERWRITE:
                # we will overwrite in place
                pass

            else:
                # Unknown policy => treat as PROMPT-equivalent refusal
                raise OverwriteError(
                    str(target),
                    f"Unknown overwrite policy: {policy}",
                )

        # Build final text/binary payload
        if entry.is_binary:
            # binary branch
            data_bytes = self._binary_payload(entry)
            if not self.dry_run:
                try:
                    target.write_bytes(data_bytes)
                except Exception as e:
                    raise BundleWriteError(
                        str(target),
                        f"Write failed: {e}",
                    )
        else:
            # text branch
            # Build canonical repo header if enabled
            final_text = (
                self._build_repo_header_block(entry) + entry.content
                if header_enabled
                else entry.content
            )

            # Output encoding choice
            encoding = (
                entry.encoding
                if entry.encoding and entry.encoding != "auto"
                else "utf-8"
            )
            if encoding.lower() in ("utf-8-bom", "utf8-bom", "utf-8_sig"):
                encoding = "utf-8-sig"

            if not self.dry_run:
                try:
                    target.write_text(final_text, encoding=encoding, newline="")
                except LookupError:
                    raise BundleWriteError(
                        str(target),
                        f"Unknown encoding: {entry.encoding}",
                    )
                except Exception as e:
                    raise BundleWriteError(
                        str(target),
                        f"Write failed: {e}",
                    )

        # update state
        self.files_written.append(target)
        self.pending_writes.add(target)

        return ("processed", str(target))

    # ------------------------------------------------------------------
    # Internal helpers
    # ------------------------------------------------------------------

    def _binary_payload(self, entry: BundleEntry) -> bytes:
        """
        Produce bytes for a binary BundleEntry, handling:
        - bytes / bytearray directly
        - base64-encoded str (decode it)
        """
        data = entry.content
        # content already bytes/bytearray
        if isinstance(data, (bytes, bytearray)):
            return bytes(data)

        # content is str: try base64 decode
        if isinstance(data, str):
            try:
                return base64.b64decode(data.strip())
            except Exception as e:
                raise BundleWriteError(
                    entry.path,
                    f"Base64 decode failed: {e}",
                )

        # unsupported type
        raise BundleWriteError(
            entry.path,
            f"Unsupported binary content type: {type(data)}",
        )

    def _get_renamed_path(self, original: Path) -> Path:
        """
        Generate a unique non-colliding alternative filename by appending
        _1, _2, ... before the suffix. Checks both filesystem and any
        already-pending writes in this run.
        """
        parent = original.parent
        stem = original.stem
        suffix = original.suffix

        counter = 1
        while True:
            candidate = parent / f"{stem}_{counter}{suffix}"
            if (not candidate.exists()) and (candidate not in self.pending_writes):
                return candidate
            counter += 1

    def _resolve_output_path(self, relative_path: str, out_dir: Path) -> Path:
        """
        Normalize an entry path to an absolute path under out_dir.

        We also do policy hardening here: reject blank, ".", "./", ".\\", "/".
        """
        rp = (relative_path or "").strip().lstrip("\\/").replace("\\", "/")

        if rp in ("", ".", "./", ".\\", "/"):
            raise BundleWriteError(
                str(out_dir),
                "Blank or dot-only file path in bundle entry",
            )

        return (out_dir / rp).resolve()

    def _validate_path(self, target_path: Path, base_dir: Path) -> None:
        """
        Ensure target_path does not escape base_dir. We preserve this method
        (and call it) because the legacy test suite asserts on it indirectly
        through extract_manifest().
        """
        resolved_target = target_path.resolve()
        resolved_base = base_dir.resolve()

        try:
            resolved_target.relative_to(resolved_base)
        except ValueError:
            raise PathTraversalError(
                str(target_path),
                f"Path would escape base directory {base_dir}",
            )

    def _build_repo_header_block(self, entry: BundleEntry) -> str:
        """
        Build the canonical repo header block for text files in extraction.

        IMPORTANT:
        - This block uses SOURCEFILE:, RELPATH:, PROJECT:, TEAM:, VERSION:,
          LIFECYCLE:, DESCRIPTION:, FIXES:
        - We ALWAYS emit all keys, in this exact order.
        - DESCRIPTION and FIXES are intentionally left blank strings,
          not omitted, to satisfy "no missing required header keywords".
        - We DO NOT emit `# FILE:` or `# META:` here. Those are transport-only.
        """

        sourcefile_val = Path(entry.path).name
        relpath_val = entry.path  # respect forward slashes as provided
        project_val = "Bundle File Tool v2.1"
        team_val = (
            "Ringo (Owner), John (Lead Dev), George (Architect), Paul (Lead Analyst)"
        )
        version_val = "2.1.0"
        lifecycle_val = "Proposed"
        description_val = ""
        fixes_val = ""

        lines = [
            "# ============================================================================",
            f"# SOURCEFILE: {sourcefile_val}",
            f"# RELPATH: {relpath_val}",
            f"# PROJECT: {project_val}",
            f"# TEAM: {team_val}",
            f"# VERSION: {version_val}",
            f"# LIFECYCLE: {lifecycle_val}",
            f"# DESCRIPTION: {description_val}",
            f"# FIXES: {fixes_val}",
            "# ============================================================================",
            "",
        ]
        return "\n".join(lines)


# -----------------------------------------------------------------------------
# BundleCreator: bundling path
# -----------------------------------------------------------------------------

class BundleCreator:
    """
    BundleCreator
    -------------
    Discovers source files from disk, applies allow/deny globs, strips any
    per-file repo headers, builds a BundleManifest, and writes that manifest
    out as a bundle text file using PlainMarkerProfile.

    Requirements this class must continue to satisfy:
    - DEFAULT_ALLOW_GLOBS / DEFAULT_DENY_GLOBS exist
    - deny globs are merged with defaults, not replaced, so .git/.venv etc.
      are ALWAYS excluded
    - discover_files() applies GlobFilter.should_include(rel_path)
      (deny precedes allow)
    - create_manifest() enforces max size, stamps metadata
    - _read_file_to_entry():
        - detects binary by NUL scan / decode attempt
        - base64 encodes binary if treat_binary_as_base64=True
        - strips header block from text via _strip_header_block()
        - detects eol style
    - bundle_manifest() writes the final .txt and logs the run
      (bundle_creation_diagnostic.log) and relies on the profile to emit
      bundle_format_diagnostic.log (transport-level log).
    """

    DEFAULT_DENY_GLOBS = [
        # VCS
        "**/.git/**",
        "**/.svn/**",
        "**/.hg/**",
        "**/.bzr/**",
        # Python caches / venvs
        "**/__pycache__/**",
        "**/.pytest_cache/**",
        "**/.mypy_cache/**",
        "**/.ruff_cache/**",
        "**/.tox/**",
        "**/venv/**",
        "**/.venv/**",
        "**/env/**",
        "**/.env/**",
        "**/*.pyc",
        "**/*.pyo",
        "**/*.pyd",
        "**/.Python",
        # Build/dist
        "**/dist/**",
        "**/build/**",
        "**/target/**",
        "**/*.egg-info/**",
        "**/*.egg/**",
        "**/*.egg",
        "**/wheels/**",
        "**/.eggs/**",
        # Coverage / test artifacts
        "**/htmlcov/**",
        "**/.coverage",
        "**/.coverage.*",
        "**/coverage.xml",
        "**/*.cover",
        # IDE/editor cruft
        "**/.vscode/**",
        "**/.idea/**",
        "**/*.swp",
        "**/*.swo",
        "**/*~",
        "**/.DS_Store",
        "**/*.sublime-project",
        "**/*.sublime-workspace",
        # Logs / temp
        "**/*.log",
        "**/logs/**",
        "**/.temp/**",
        "**/tmp/**",
        "**/.tmp/**",
        # Node/JS
        "**/node_modules/**",
        "**/.next/**",
        "**/.nuxt/**",
        # general cache
        "**/.cache/**",
        # prevent recursive bundling of generated bundle artifacts
        "**/project.txt",
        "**/*_bundle.txt",
        "**/bundle_*.txt",
        # OS specific junk
        "**/Thumbs.db",
        "**/.Trashes",
        "**/ehthumbs.db",
    ]

    DEFAULT_ALLOW_GLOBS = [
        # source code
        "**/*.py",
        "**/*.pyx",
        "**/*.pxd",
        # docs
        "**/*.md",
        "**/*.rst",
        "**/*.txt",
        # config
        "**/*.json",
        "**/*.toml",
        "**/*.yaml",
        "**/*.yml",
        "**/*.ini",
        "**/*.cfg",
        "**/*.conf",
        # web
        "**/*.html",
        "**/*.htm",
        "**/*.css",
        "**/*.scss",
        "**/*.sass",
        "**/*.less",
        "**/*.js",
        "**/*.ts",
        "**/*.jsx",
        "**/*.tsx",
        # data
        "**/*.xml",
        "**/*.csv",
        "**/*.sql",
        # scripts
        "**/*.sh",
        "**/*.bash",
        "**/*.bat",
        "**/*.ps1",
        # build files
        "**/Makefile",
        "**/*.mk",
        "**/Dockerfile",
        "**/.dockerignore",
        "**/.gitignore",
    ]

    def __init__(
        self,
        allow_globs: Optional[List[str]] = None,
        deny_globs: Optional[List[str]] = None,
        max_file_mb: float = 10.0,
        treat_binary_as_base64: bool = True,
    ):
        # allow_globs: if None use defaults, if [] means "everything"
        self.allow_globs = (
            self.DEFAULT_ALLOW_GLOBS.copy()
            if allow_globs is None
            else allow_globs
        )

        # deny_globs: must merge with defaults
        if deny_globs is None:
            self.deny_globs = self.DEFAULT_DENY_GLOBS.copy()
        else:
            merged = set(self.DEFAULT_DENY_GLOBS)
            merged.update(deny_globs)
            self.deny_globs = sorted(list(merged))

        self.max_file_mb = max_file_mb
        self.treat_binary_as_base64 = treat_binary_as_base64

        # bundle creation diagnostic logger
        self._blog = logging.getLogger("bundle.creation_diagnostic")
        self._blog.setLevel(logging.DEBUG)
        if not self._blog.handlers:
            fh = logging.FileHandler(
                "bundle_creation_diagnostic.log",
                mode="w",
                encoding="utf-8",
            )
            fh.setLevel(logging.DEBUG)
            fmt = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
            fh.setFormatter(fmt)
            self._blog.addHandler(fh)
            self._blog.propagate = False

        # profile responsible for formatting the manifest into bundle text
        self.profile = PlainMarkerProfile()

    def discover_files(
        self, source_path: Path, base_path: Optional[Path] = None
    ) -> List[Path]:
        """
        Recursively discover files under source_path using allow/deny globs.
        Uses GlobFilter.should_include(rel_path). deny_globs take precedence.
        """
        from core.validators import GlobFilter  # local import to avoid cycle

        sp = Path(source_path)
        if not sp.exists():
            raise BundleWriteError(str(sp), "Source path does not exist")

        # Single-file mode
        if sp.is_file():
            rel_path = None
            if base_path:
                try:
                    rel_path = str(sp.relative_to(base_path)).replace("\\", "/")
                except ValueError:
                    rel_path = sp.name
            else:
                rel_path = sp.name

            gf = GlobFilter(
                allow_patterns=self.allow_globs,
                deny_patterns=self.deny_globs,
            )
            return [sp] if gf.should_include(rel_path) else []

        # Directory mode
        base = base_path or sp
        gf = GlobFilter(
            allow_patterns=self.allow_globs,
            deny_patterns=self.deny_globs,
        )

        discovered: List[Path] = []
        for p in sp.rglob("*"):
            if not p.is_file():
                continue

            try:
                rel_path = str(p.relative_to(base)).replace("\\", "/")
            except ValueError:
                rel_path = p.name

            if gf.should_include(rel_path):
                discovered.append(p)

        return discovered

    def create_manifest(
        self,
        files: List[Path],
        base_path: Path,
        profile_name: str,
    ) -> BundleManifest:
        """
        Build a BundleManifest from a list of file paths.
        - Enforces max file size
        - Strips per-file headers from text content
        - Detects binary vs text
        - Base64 encodes binary if treat_binary_as_base64 is True
        - Records metadata (created timestamp, file_count, etc.)
        """
        entries: List[BundleEntry] = []

        for file_path in files:
            try:
                rel_path_obj = file_path.relative_to(base_path)
                rel_path = str(rel_path_obj)
            except ValueError:
                # fallback if file is not under base_path
                rel_path = file_path.name

            size_mb = file_path.stat().st_size / (1024 * 1024)
            if size_mb > self.max_file_mb:
                raise FileSizeError(str(rel_path), size_mb, self.max_file_mb)

            entry = self._read_file_to_entry(file_path, rel_path)
            entries.append(entry)

        return BundleManifest(
            entries=entries,
            profile=profile_name,
            metadata={
                "created": datetime.now().isoformat(),
                "source_path": str(base_path),
                "file_count": len(entries),
            },
        )

    def bundle_manifest(
        self,
        manifest: BundleManifest,
        bundle_path: Path,
    ) -> None:
        """
        Serialize the manifest to a bundle text file using PlainMarkerProfile
        and write it to disk. Also logs diagnostic info and relies on the
        profile to write bundle_format_diagnostic.log.
        """
        self._blog.info("=" * 80)
        self._blog.info("START bundle_manifest()")
        self._blog.info("Bundle path: %s", bundle_path)
        self._blog.info("Entries: %d", len(manifest.entries))

        try:
            text_blob = self.profile.format_manifest(manifest)
            bundle_path = Path(bundle_path)
            bundle_path.parent.mkdir(parents=True, exist_ok=True)
            with open(bundle_path, "w", encoding="utf-8", newline="") as outf:
                outf.write(text_blob)
            self._blog.info(
                "Bundle write OK (%d chars)", len(text_blob)
            )
        except Exception as exc:
            self._blog.exception(
                "Error during bundle creation: %s", exc
            )
            raise
        finally:
            self._blog.info("END bundle_manifest()")
            self._blog.info("=" * 80)

    # ------------------------------------------------------------------
    # Source file reading helpers
    # ------------------------------------------------------------------

    def _read_file_to_entry(
        self, file_path: Path, rel_path: str
    ) -> BundleEntry:
        """
        Read file_path from disk and create a BundleEntry:
        - Detect binary via NUL check + decode attempt
        - For text: strip canonical/legacy header block before capturing content
        - For binary: base64 encode (if allowed)
        - Detect eol style
        """
        file_path = Path(file_path)

        # binary detect
        try:
            with open(file_path, "rb") as f:
                probe = f.read(1024)
                if b"\x00" in probe:
                    is_binary = True
                else:
                    # try decoding as utf-8
                    probe.decode("utf-8")
                    is_binary = False
        except (UnicodeDecodeError, TypeError):
            is_binary = True
        except Exception:
            # if we somehow can't read, default to text=false? safer to default text
            is_binary = False

        if is_binary:
            if not self.treat_binary_as_base64:
                raise BundleWriteError(
                    str(rel_path),
                    "Binary file found but base64 handling disabled.",
                )
            raw_bytes = file_path.read_bytes()
            b64_content = base64.b64encode(raw_bytes).decode("ascii")
            return BundleEntry(
                path=rel_path.replace("\\", "/"),
                content=b64_content,
                is_binary=True,
                encoding="base64",
                eol_style="n/a",
                checksum=None,
            )

        # text case
        text_raw = file_path.read_text(encoding="utf-8")
        text_clean = self._strip_header_block(text_raw)

        return BundleEntry(
            path=rel_path.replace("\\", "/"),
            content=text_clean,
            is_binary=False,
            encoding="utf-8",
            eol_style=self._detect_eol(text_clean),
            checksum=None,
        )

    def _strip_header_block(self, content: str) -> str:
        """
        Strip any leading repo header block from a source file *before* it
        goes into the bundle. This supports BOTH:
        - canonical repo header (SOURCEFILE:, RELPATH:, PROJECT:, TEAM:, ...)
        - legacy transport header (# FILE:, # META:)

        Rules:
        - Header must start with a border line that looks like
          "# ========" (only '#', '=', and spaces).
        - We then scan consecutive '#' lines. If any of them contain
          a colon "KEY:" (SOURCEFILE:, RELPATH:, PROJECT:, TEAM:, VERSION:,
          LIFECYCLE:, DESCRIPTION:, FIXES:, FILE:, META: etc.), we mark that
          we are inside a metadata header.
        - Header ends at the next border or at the first non-comment line.
        - If we never see a metadata-ish "KEY:" line, we consider it not
          a managed header and we leave the file unchanged.

        This matches the flexibility requirement:
        - Missing DESCRIPTION: or FIXES: is okay
        - Extra keys are okay
        - Completely missing header is okay (we just return content)
        """

        if not content.strip():
            return content

        lines = content.splitlines(keepends=True)
        if not lines:
            return content

        def is_border(txt: str) -> bool:
            s = txt.strip()
            return (
                s.startswith("#")
                and "=" in s
                and all(ch in {"#", "=", " "} for ch in s)
            )

        # first line must be border or we bail
        if not is_border(lines[0]):
            return content

        saw_metadata = False
        header_end_idx = None

        for i, raw in enumerate(lines[1:], start=1):
            stripped = raw.strip()

            # next border ends header (inclusive) if we already saw metadata
            if is_border(stripped) and saw_metadata:
                header_end_idx = i + 1
                break

            if stripped.startswith("#"):
                after_hash = stripped.lstrip("#").strip()
                if after_hash == "":
                    # blank comment line continues header
                    continue
                if ":" in after_hash:
                    # KEY: ... style line => definitely metadata
                    saw_metadata = True
                    continue
                # comment with no ":":
                if saw_metadata:
                    # still header, continue
                    continue
                # comment before any metadata: could just be a normal comment
                return content
            else:
                # first non-comment line
                if saw_metadata:
                    header_end_idx = i
                    break
                else:
                    # we never saw metadata -> treat whole thing as code
                    return content

        if header_end_idx is None:
            if saw_metadata:
                header_end_idx = len(lines)
            else:
                return content

        return "".join(lines[header_end_idx:])

    def _detect_eol(self, text: str) -> str:
        """
        Detect line ending style.
        Returns "CRLF", "LF", "CR", or "MIXED".
        Defaults to "LF" if unclear.
        """
        # check mixed endings explicitly
        if "\r\n" in text and (
            "\n" in text.replace("\r\n", "")
            or "\r" in text.replace("\r\n", "")
        ):
            return "MIXED"
        if "\n" in text and "\r" in text and "\r\n" not in text:
            return "MIXED"

        if "\r\n" in text:
            return "CRLF"
        if "\n" in text:
            return "LF"
        if "\r" in text:
            return "CR"

        return "LF"
